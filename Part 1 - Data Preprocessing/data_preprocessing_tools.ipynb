{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WOw8yMd1VlnD"},"source":["# Data Preprocessing Template"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NvUGC8QQV6bV"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"wfFEXZC0WS-V"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fhYaZ-ENV_c5"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{},"colab_type":"code","id":"aqHTg9bxWT_u"},"outputs":[],"source":["dataset = pd.read_csv('Data.csv')\n","x = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["array([['France', 44.0, 72000.0],\n","       ['Spain', 27.0, 48000.0],\n","       ['Germany', 30.0, 54000.0],\n","       ['Spain', 38.0, 61000.0],\n","       ['Germany', 40.0, nan],\n","       ['France', 35.0, 58000.0],\n","       ['Spain', nan, 52000.0],\n","       ['France', 48.0, 79000.0],\n","       ['Germany', 50.0, 83000.0],\n","       ['France', 37.0, 67000.0]], dtype=object)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n","      dtype=object)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"markdown","metadata":{},"source":["TAKING CARE OF MISSING DATA"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n","imputer.fit(x[:, 1:])\n","x[:, 1:] = imputer.transform(x[:, 1:])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["array([['France', 44.0, 72000.0],\n","       ['Spain', 27.0, 48000.0],\n","       ['Germany', 30.0, 54000.0],\n","       ['Spain', 38.0, 61000.0],\n","       ['Germany', 40.0, 63777.77777777778],\n","       ['France', 35.0, 58000.0],\n","       ['Spain', 38.77777777777778, 52000.0],\n","       ['France', 48.0, 79000.0],\n","       ['Germany', 50.0, 83000.0],\n","       ['France', 37.0, 67000.0]], dtype=object)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"markdown","metadata":{},"source":["## Encoding Categorical Data"]},{"cell_type":"markdown","metadata":{},"source":["Getting started in applied machine learning can be difficult, especially when working with real-world data.\n","\n","Often, machine learning tutorials will recommend or require that you prepare your data in specific ways before fitting a machine learning model.\n","\n","One good example is to use a one-hot encoding on categorical data.\n","\n","\n","What is Categorical Data?\n","Categorical data are variables that contain label values rather than numeric values.\n","\n","The number of possible values is often limited to a fixed set.\n","\n","Categorical variables are often called nominal.\n","\n","Some examples include:\n","\n","A “pet” variable with the values: “dog” and “cat“.\n","\n","A “color” variable with the values: “red“, “green” and “blue“.\n","\n","A “place” variable with the values: “first”, “second” and “third“.\n","\n","Each value represents a different category.\n","\n","Some categories may have a natural relationship to each other, such as a natural ordering.\n","\n","The “place” variable above does have a natural ordering of values. This type of categorical variable is called an ordinal variable.\n","\n","\n","What is the Problem with Categorical Data?\n","Some algorithms can work with categorical data directly.\n","\n","For example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation).\n","\n","Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n","\n","In general, this is mostly a constraint of the efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves.\n","\n","This means that categorical data must be converted to a numerical form. If the categorical variable is an output variable, you may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application.\n","\n","How to Convert Categorical Data to Numerical Data?\n","This involves two steps:\n","\n","- Integer Encoding\n","- One-Hot Encoding\n","\n","1. Integer Encoding\n","As a first step, each unique category value is assigned an integer value.\n","\n","For example, “red” is 1, “green” is 2, and “blue” is 3.\n","\n","This is called a label encoding or an integer encoding and is easily reversible.\n","\n","For some variables, this may be enough.\n","\n","The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n","\n","For example, ordinal variables like the “place” example above would be a good example where a label encoding would be sufficient.\n","\n","2. One-Hot Encoding\n","For categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n","\n","In fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n","\n","In this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n","\n","In the “color” variable example, there are 3 categories and therefore 3 binary variables are needed. A “1” value is placed in the binary variable for the color and “0” values for the other colors.\n","\n","For example:\n","\n","red,\tgreen,\tblue\n","\n","1,\t\t0,\t\t0 \n","\n","0,\t\t1,\t\t0\n","\n","0,\t\t0,\t\t1\n","\n","The binary variables are often called “dummy variables” in other fields, such as statistics.\n","\n","For a step-by-step tutorial on how to one hot encode categorical data in Python, see the tutorial:\n","https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/\n","\n","\n","Further Reading\n","Categorical variable on Wikipedia\n","Nominal category on Wikipedia\n","Dummy variable on Wikipedia\n","\n","Summary\n","In this post, you discovered why categorical data often must be encoded when working with machine learning algorithms.\n","\n","Specifically:\n","\n","That categorical data is defined as variables with a finite set of label values.\n","That most machine learning algorithms require numerical input and output variables.\n","That an integer and one hot encoding is used to convert categorical data to integer data.\n","\n","src - https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"]},{"cell_type":"markdown","metadata":{},"source":["### Encoding Independent Variable"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Here the independent catergorical column is country.\n","\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n","x = np.array(ct.fit_transform(X=x))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n","       [0.0, 0.0, 1.0, 27.0, 48000.0],\n","       [0.0, 1.0, 0.0, 30.0, 54000.0],\n","       [0.0, 0.0, 1.0, 38.0, 61000.0],\n","       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n","       [1.0, 0.0, 0.0, 35.0, 58000.0],\n","       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n","       [1.0, 0.0, 0.0, 48.0, 79000.0],\n","       [0.0, 1.0, 0.0, 50.0, 83000.0],\n","       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"markdown","metadata":{},"source":["### Encoding the dependent Variable"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","y = le.fit_transform(y)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3abSxRqvWEIB"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{},"colab_type":"code","id":"hm48sif-WWsh"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1.0, 0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n","       [0.0, 1.0, 0.0, 0.0, 37.0, 67000.0],\n","       [1.0, 0.0, 0.0, 1.0, 27.0, 48000.0],\n","       [1.0, 0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n","       [0.0, 1.0, 0.0, 0.0, 48.0, 79000.0],\n","       [1.0, 0.0, 0.0, 1.0, 38.0, 61000.0],\n","       [0.0, 1.0, 0.0, 0.0, 44.0, 72000.0],\n","       [0.0, 1.0, 0.0, 0.0, 35.0, 58000.0]], dtype=object)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Scaling"]},{"cell_type":"markdown","metadata":{},"source":["Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. Just to give you an example — if you have multiple independent variables like age, salary, and height; With their range as (18–100 Years), (25,000–75,000 Euros), and (1–2 Meters) respectively, feature scaling would help them all to be in the same range, for example- centered around 0 or in the range (0,1) depending on the scaling technique."]},{"cell_type":"markdown","metadata":{},"source":["For some models, to avoid some features to be dominating over other features leading to some features not even being considered by the model"]},{"cell_type":"markdown","metadata":{},"source":["Specifically, in the case of Neural Networks Algorithms, feature scaling benefits optimization by:\n","\n","It makes the training faster\n","\n","It prevents the optimization from getting stuck in local optima\n","\n","It gives a better error surface shape\n","\n","Weight decay and Bayes optimization can be done more conveniently\n"]},{"cell_type":"markdown","metadata":{},"source":["Standardization -\n","\n","![standardization](https://www.atoti.io/wp-content/uploads/2021/01/1_6TnJHNoeu9ZSni1B94MZow.png)\n","\n","Normalization -\n","\n","![Normalization](https://www.atoti.io/wp-content/uploads/2021/01/1_lz4NqpzsmNH9bvcZttvcCg.png)"]},{"cell_type":"markdown","metadata":{},"source":["The million-dollar question: Normalization or Standardization\n","If you have ever built a machine learning pipeline, you must have always faced this question of whether to Normalize or to Standardize. While there is no obvious answer to this question, it really depends on the application, there are still a few generalizations that can be drawn.\n","\n","Normalization is good to use when the distribution of data does not follow a Gaussian distribution. It can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors.\n","\n","In Neural Networks algorithm that require data on a 0–1 scale, normalization is an essential pre-processing step. Another popular example of data normalization is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range).\n","\n","Standardization can be helpful in cases where the data follows a Gaussian distribution. Though this does not have to be necessarily true. Since standardization does not have a bounding range, so, even if there are outliers in the data, they will not be affected by standardization.\n","\n","In clustering analyses, standardization comes in handy to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling since we are interested in the components that maximize the variance.\n","\n","There are some points which can be considered while deciding whether we need Standardization or Normalization\n","\n","Standardization may be used when data represent Gaussian Distribution, while Normalization is great with Non-Gaussian Distribution\n","Impact of Outliers is very high in Normalization\n","To conclude, you can always start by fitting your model to raw, normalized, and standardized data and compare the performance for the best results.\n","\n","ref - https://www.atoti.io/when-to-perform-a-feature-scaling/"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler()\n","X_train[:,3:] = ss.fit_transform(X_train[:,3:])\n","X_test[:, 3:] = ss.transform(X_test[:, 3:])"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.0, 1.0, 0.0, 0.2630675731713538, 0.1238147854838185],\n","       [1.0, 0.0, 0.0, -0.25350147960148617, 0.4617563176278856],\n","       [0.0, 0.0, 1.0, -1.9753983221776195, -1.5309334063940294],\n","       [0.0, 0.0, 1.0, 0.05261351463427101, -1.1114197802841526],\n","       [1.0, 0.0, 0.0, 1.6405850472322605, 1.7202971959575162],\n","       [0.0, 0.0, 1.0, -0.08131179534387283, -0.16751412153692966],\n","       [1.0, 0.0, 0.0, 0.9518263102018072, 0.9861483502652316],\n","       [1.0, 0.0, 0.0, -0.5978808481167128, -0.48214934111933727]],\n","      dtype=object)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["array([[0.0, 1.0, 0.0, -1.4588292694047795, -0.9016629672292141],\n","       [0.0, 1.0, 0.0, 1.984964415747487, 2.139810822067393]],\n","      dtype=object)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["X_test"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOD2/gZgY69JdiiGJVNfu7s","collapsed_sections":[],"name":"data_preprocessing_template.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"8b553e33a5ecb7a079b10de35e578a314f00b5e3483a8477375a33f8165837d0"},"kernelspec":{"display_name":"Python 3.7.0 64-bit ('env': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}
